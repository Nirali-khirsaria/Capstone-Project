# -*- coding: utf-8 -*-
"""EDA_Group_6_Capestone_Final_SEM_4_Krupali (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E8euCs_qcx4J8oRau3rrFbTD93m-vkZg

### Drive mount for dataset access
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
!pip install scikit-learn

"""### Import Dataset"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sem-4/capstone/Kenya_Data.csv')

"""### **Dataset Overview:**

Let's take a look at the first few rows of your dataset:
"""

df.T

df.isnull().sum()

df.dtypes

# df['Order Date'] = pd.to_datetime(df['Order Date'])
# df.dtypes

# drop_coumns = ['Coupon Code', 'Discount Amount', 'Item #', 'Discount Amount Tax','Last Name (Billing)', 'Company (Billing)', 'Address 1&2 (Billing)', 'Postcode (Billing)','Country Code (Billing)', 'Customer Note', 'Email (Billing)', 'Phone (Billing)', 'Last Name (Shipping)', 'Address 1&2 (Shipping)', 'State Code (Shipping)','Postcode (Shipping)', 'Country Code (Shipping)', 'Cart Discount Amount', 'Order Total Tax Amount']
# df = df.drop(columns=drop_coumns)

# df

"""### Checking NaN Values in Data"""

nan_columns = df.isnull().sum()
print("Columns with NaN values:")
print(nan_columns[nan_columns > 0])

"""## Handled NaN Values as 0 (Zero)"""

columns_to_replace_with_zero = ['Shipping Method Title', 'SKU']

df[columns_to_replace_with_zero] = df[columns_to_replace_with_zero].fillna(0)

df.T

companies = ['Apple', 'Cursor', 'HP', 'Nokia', 'Oppo', 'Samsung', 'Epson', 'Acer', 'Von', 'Hikvision', 'Toshiba', 'Lexar', 'Display', 'Crucial', 'Vision Plus', 'Sony', 'Mi', 'Transcend', 'Infinix', 'Realme', 'Oneplus', 'Google', 'LG', 'EcoTank', 'Test', 'iTel', 'Vivo', 'Xiaomi', ]
df['Company'] = df['Item Name'].apply(lambda x: next((company for company in companies if company in x), 'Other_Company'))

df.head().T

"""### **Descriptive Statistics:**

Use summary statistics like mean, median, mode, range, and standard deviation to describe the central tendency and spread of numeric features.
"""

df.describe()

"""### **Handling Missing Data:**

Identify and handle missing data. You can check for missing values in your dataset using the following:
"""

# Check for missing values
df.isnull().sum()

"""### *Correlation Analysis:*

Interpretation of the correlation matrix:



*   A value close to 1 indicates a strong positive correlation.
*   A value close to -1 indicates a strong negative correlation.
*   A value close to 0 indicates a weak or no correlation.

"""

# correlation_matrix = df.corr()

# # Plotting the correlation matrix as a heatmap
# plt.figure(figsize=(12, 10))
# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
# plt.title('Correlation Matrix')
# plt.show()

"""### Box Plot for Outlier Detection:"""

# Box Plot with Outliers

plt.figure(figsize=(8, 6))
ax = sns.boxplot(x=df['Order Total Amount'], color='salmon')

# Customize x-axis ticks
tick_values = np.arange(0, df['Order Total Amount'].max() + 1000000, 1000000)
ax.set_xticks(tick_values)
ax.set_xticklabels([f'{val:,}' for val in tick_values])  # Format ticks with commas for better readability

# Set plot title and labels
plt.title('Box Plot: Order Total Amount with Outliers')
plt.xlabel('Order Total Amount')
plt.show()

"""### Removing Outliers :"""

Q1 = df['Order Total Amount'].quantile(0.25)
Q3 = df['Order Total Amount'].quantile(0.75)
IQR = Q3 - Q1
df_no_outliers = df[(df['Order Total Amount'] >= Q1 - 1.5 * IQR) & (df['Order Total Amount'] <= Q3 + 1.5 * IQR)]


plt.figure(figsize=(8, 6))
sns.boxplot(x=df_no_outliers['Order Total Amount'],showfliers=False, color='salmon')
plt.title('Box Plot: Order Total Amount after removed Outliers ')
plt.xlabel('Order Total Amount')
plt.show()

"""### Univariate Analysis for Numeric Variables:"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Order Total Amount'], bins=30, kde=True, color='skyblue', stat='count')

plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))

plt.title('Histogram of Order Total Amount')
plt.xlabel('Order Total Amount')
plt.ylabel('Count')
plt.show()

"""### Univariate Analysis for Categorical Variables:"""

plt.figure(figsize=(8, 6))
sns.countplot(x=df['Order Status'], palette='viridis')
plt.title('Order Status Distribution')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x='Payment Method Title', data=df, palette='magma')
plt.title('Payment Method Distribution')
plt.xlabel('Payment Method')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

top_products = df.groupby('Item Name')['Quantity'].sum().nlargest(10)
plt.figure(figsize=(12, 6))
top_products.plot(kind='bar', color='skyblue')
plt.title('Top Selling Products')
plt.xlabel('Product')
plt.ylabel('Total Quantity Sold')
plt.show()

"""### Bivariate Analysis for Numeric-Numeric Relationships:"""

plt.figure(figsize=(12, 8))
sns.violinplot(x='Order Shipping Amount', y='Quantity', data=df, palette='viridis')
plt.title('Violin Plot: Distribution of Quantity for different Order Shipping Amount levels')
plt.xlabel('Order Shipping Amount')
plt.ylabel('Quantity')
plt.show()

"""### Bivariate Analysis for Categorical-Categorical Relationships:"""

cross_tab = pd.crosstab(df['Order Status'], df['Payment Method Title'])
plt.figure(figsize=(12, 8))
sns.heatmap(cross_tab, annot=True, cmap='YlGnBu', fmt='d', cbar_kws={'label': 'Count'})
plt.title('Cross-Tabulation: Order Status vs Payment Method Title')
plt.xlabel('Payment Method Title')
plt.ylabel('Order Status')
plt.show()

"""### Bivariate Analysis for Numeric-Categorical Relationships:"""

plt.figure(figsize=(12, 8))
sns.barplot(x='Order Status', y='Order Total Amount', data=df, ci=None, palette='pastel')
plt.title('Bar Plot: Average Order Total Amount by Order Status')
plt.xlabel('Order Status')
plt.ylabel('Average Order Total Amount')
plt.show()

"""### Analyzing Sales Over Time:"""

# df['Order Date'] = pd.to_datetime(df['Order Date'])
# df.set_index('Order Date', inplace=True)

# # Resample data to monthly and sum the total sales
# monthly_sales = df['Order Total Amount'].resample('M').sum()

# plt.figure(figsize=(12, 6))
# monthly_sales.plot(kind='line', marker='o', color='green')
# plt.title('Monthly Total Sales Over Time')
# plt.xlabel('Month')
# plt.ylabel('Total Sales')

# # Format y-axis labels as integers
# plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))

# plt.show()

Cancelled_orders = df[df['Order Status'] == 'Cancelled']
top_Cancelled_products = Cancelled_orders['Item Name'].value_counts().head(10)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_Cancelled_products.values, y=top_Cancelled_products.index, palette='viridis')
plt.title('Top 10 Cancelled Products')
plt.xlabel('Count')
plt.ylabel('Product Name')
plt.show()

company_total_sales = df.groupby('Company').size().reset_index(name='Total Sales')
company_total_sales = company_total_sales.sort_values(by='Total Sales', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Total Sales', y='Company', data=company_total_sales)
plt.title('Total Sales by Company')
plt.xlabel('Total Sales')
plt.ylabel('Company')
plt.show()

df.T

df['Profit'] = (df['Order Subtotal Amount'])- (df['Item Cost'])

df.head()

from sklearn.preprocessing import LabelEncoder
order_status_mapping = {
    'Cancelled': 0,
    'Completed': 1,
    'failed': 2,
    'Cancel request': 0,
    'on hold': 3,
    'processing': 4
}

label_encoder = LabelEncoder()
df['Order Status Numerical'] = label_encoder.fit_transform(df['Order Status'].map(order_status_mapping))
print(df[['Order Status', 'Order Status Numerical']])

# Reset index to make 'Order Date' a regular column
df.reset_index(inplace=True)

# Now 'Order Date' will be a regular column in the DataFrame

# correlation_matrix = df.corr()
# plt.figure(figsize=(12, 10))
# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
# plt.title('Correlation Matrix')
# plt.show()

df.describe()

df.dtypes

#df.to_csv('/content/drive/MyDrive/modified_dataset.csv', index=False)
#from google.colab import files
#files.download('/content/drive/MyDrive/modified_dataset.csv')

import pandas as pd

# Group by 'City (Billing)' and 'City (Shipping)' columns and perform aggregation
grouped_data = df.groupby(['City (Billing)', 'City (Shipping)']).agg({'Order Number': 'count'}).reset_index()
grouped_data.rename(columns={'Order Number': 'Order_Count'}, inplace=True)
df = pd.merge(df, grouped_data, on=['City (Billing)', 'City (Shipping)'], how='left')

df.head()

df.T

df.dtypes

pip install category_encoders

import pandas as pd
from sklearn.preprocessing import LabelEncoder

def frequency_encode(column):
    frequency_map = column.value_counts(normalize=True)
    return column.map(frequency_map)

categorical_columns = ['Company', 'City (Billing)', 'Category Name', 'City (Shipping)']
for col in categorical_columns:
    df[col+'_encoded'] = frequency_encode(df[col])

print(df.head())

df.dtypes

# Select numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64', 'datetime64[ns]']).columns

# Select categorical columns
categorical_columns = df.select_dtypes(include=['object']).columns

print("Numerical Columns:")
print(numerical_columns)
print("\nCategorical Columns:")
print(categorical_columns)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


columns_for_correlation = [
    'Order Number', 'Order Subtotal Amount', 'Order Shipping Amount',
       'Order Total Amount', 'Quantity', 'Item Cost', 'Profit',
       'Order Status Numerical', 'Order_Count', 'Company_encoded',
       'City (Billing)_encoded', 'Category Name_encoded'
]


correlation_matrix = df[columns_for_correlation].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# Select numerical columns
numerical_columns = [
    'Order Number', 'Order Subtotal Amount', 'Order Shipping Amount',
       'Order Total Amount', 'Quantity', 'Item Cost',
       'Order Status Numerical', 'Order_Count', 'Company_encoded',
       'City (Billing)_encoded', 'Category Name_encoded'
]

# Features
X = df[numerical_columns]

# Target variable
y = df['Profit']

# Initialize RandomForestRegressor
rf = RandomForestRegressor()

# Fit the model
rf.fit(X, y)

# Get feature importances
feature_importances = rf.feature_importances_

# Creating DataFrame to store feature importance
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sorting the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plotting feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

import pandas as pd
# List of float64 columns
float64_columns = ['Company_encoded', 'City (Billing)_encoded', 'Category Name_encoded', 'City (Shipping)_encoded','Order Total Amount']

# Convert float64 columns to int64
df[float64_columns] = df[float64_columns].astype('int64')
print(df.dtypes)

import pandas as pd

# Drop 'Order Total Amount' and 'Order Shipping Amount' columns
df.drop(['Order Total Amount', 'Order Shipping Amount'], axis=1, inplace=True)

# Rename 'Order Subtotal Amount' column to 'Order Amount'
df.rename(columns={'Order Subtotal Amount': 'Order Amount'}, inplace=True)

# Display the modified dataframe
print(df)

df.columns

# import pandas as pd

# # Assuming 'df' is your DataFrame containing the 'Order Date' column
# # Convert 'Order Date' column to datetime type
# df['Order Date'] = pd.to_datetime(df['Order Date'])

# # Remove the time component and keep only the date
# df['Order Date'] = df['Order Date'].dt.date

# # Now the 'Order Date' column will contain only the date component without the time
# df.head()

# Separate int64 columns
df_numerical = df.select_dtypes(include=['int64', 'datetime64[ns]'])

# Separate object columns
df_cat = df.select_dtypes(include=['object'])

df_numerical.T

# !pip install lazypredict
#!pip uninstall scikit-learn==1.3.0

# from lazypredict.Supervised import LazyRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error

# X = df_numerical.drop(columns=['Profit'])
# y = df_numerical['Profit']

# # Splitting the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Initialize LazyRegressor
# reg = LazyRegressor(predictions=True)

# # Fit and predict using LazyRegressor
# models, predictions = reg.fit(X_train, X_test, y_train, y_test)

# # Print the R-squared score for each model
# print(models)

# Selecting only the specified columns
selected_columns = ['Order Amount', 'Item Cost', 'Quantity','Profit']
df_selected = df_numerical[selected_columns]

# Now, df_selected contains only the columns 'Order Amount', 'Quantity', 'Item Cost', and 'Order Status Numerical'
df_selected

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from math import sqrt

# Assuming your target variable is 'Profit' or any other target you want to predict
target_variable = 'Profit'

# Assuming 'int64_df' is your DataFrame containing only int64 columns
X = df_selected.drop(columns=[target_variable])
y = df_selected[target_variable]

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a list of regressors to loop through
regressors = [
    ('Extra Trees Regressor', ExtraTreesRegressor()),
    ('Gradient Boosting Regressor', GradientBoostingRegressor()),
    ('Random Forest Regressor', RandomForestRegressor()),
    ('XGB Regressor', XGBRegressor()),
    ('Bagging Regressor', BaggingRegressor()),
    ('Decision Tree Regressor', DecisionTreeRegressor())
]

# Iterate over regressors
for reg_name, reg in regressors:
    # Train the regressor
    reg.fit(X_train, y_train)

    # Predictions on the test set
    y_pred = reg.predict(X_test)

    # Calculate evaluation metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = sqrt(mse)  # Calculate RMSE
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Print the evaluation metrics for each model
    print(f"Evaluation metrics for {reg_name}:")
    print(f"Mean Squared Error: {mse: .4f}")
    print(f"Root Mean Squared Error: {rmse: .4f}")  # Print RMSE
    print(f"Mean Absolute Error: {mae: .4f}")
    print(f"R-squared Score: {r2: .4f}")
    print("--------------------------------------------------------")

X_train

df_selected.columns

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
# Initialize the Extra Trees Regressor
regressor = RandomForestRegressor()

# Fit the model on the training data
regressor.fit(X_train, y_train)

# Make predictions on the test data
predictions = regressor.predict(X_test)

# Calculate evaluation metrics
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Print the evaluation metrics rounded to two decimal places
print("Root Mean Squared Error:", round(rmse, 2))
print("Mean Absolute Error:", round(mae, 2))
print("R-squared Score:", round(r2, 2))

input_data = (4000, 2000, 1)
input_data_numpyarray = np.asarray(input_data)
input_reshape = input_data_numpyarray.reshape(1, -1)
prediction = regressor.predict(input_reshape)

#print(prediction)

if (prediction[0] == 0):
  print("No Profit make")
else:
  print("Profit Made", prediction)

# df_selected.to_csv('/content/drive/MyDrive/modified_dataset.csv', index=False)
# from google.colab import files
# files.download('/content/drive/MyDrive/modified_dataset.csv')

